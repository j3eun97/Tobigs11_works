{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "서술형\n",
    "1.He 초기화를 사용하여 무작위로 선택한 값이라면 모든 가중치를 같은 값으로 초기화해도 괜찮을까요?\n",
    "모든 가중치가 동일한 값으로 시작되고 솔루션이 불평등한 가중치를 개발하도록 요구하면 시스템은 결코 배울 수 없다.\n",
    "이는 오차가 가중치의 값에 비례하여 가중치를 통해 다시 전파되기 때문이다. 즉, 출력 단위에 직접 연결된 모든 숨겨진 \n",
    "단위는 동일한 오류 신호를 수신하며, 중량 변화는 오류 신호에 따라 달라지기 때문에 해당 단위에서 출력단위로의 가중치는 \n",
    "항상 동일해야 한다. 체계는 가중치를 균등하게 유지하는 일종의 불안정한 평형점에서 출발하고 있지만 오차 표면의 일부 인접\n",
    "지점보다 높고, 일단 이 지점들 중 하나로 이동하면 결코 되돌아오지 않는다. 우리는 시스템을 작은 무작위 가중치로 시작하여 \n",
    "이 문제에 대응한다. 이런 조건에서는 이런 종류의 대칭 문제가 발생하지 않는다.\n",
    "\n",
    "2.ReLU는 왜 좋은가요? ReLU보다 ELU가 나은 점은 무엇인가요?\n",
    "Advantages of Relu\n",
    "-not vanishing gradient\n",
    "-More computationally efficient to compute than Sigmoid like functions since Relu just needs to pick max(0,x) and \n",
    "not perform expensive exponential operations as in Sigmoids\n",
    "-In practice, networks with Relu tend to show better convergence performance than sigmoid. (Krizhevsky et al.)\n",
    "\n",
    "ELU는 음의 입력을 제외하고 RELU와 매우 유사하다. 그들은 둘 다 음이 아닌 입력에 대한 ID 함수 형식이다. \n",
    "반면, ELU는 출력이 -α와 같아질 때까지 천천히 매끄러워지는 반면, RELU는 날카롭게 매끄러워진다. \n",
    "Relu와 달리 ELU는 음의 출력을 낼 수 있다. 자료의 소실이 줄어든다.\n",
    "\n",
    "3.어떤 경우에 ELU, LeakyReLU, ReLU, tanh, sigmoid, softmax같은 활성화 함수로 사용해야 하나요?\n",
    "ELU: When the synapse activity is zero it makes sense that the derivative of the activation function is zero \n",
    "because there is no need to update as the synapse was not used. Furthermore, if the value is lower than zero, \n",
    "the resulting derivative will be also zero leading to a disconnection of the neuron (no update). \n",
    "This is a good idea since disconnecting some neurons may reduce overfitting (as co-dependence is reduced), \n",
    "however this will hinder the neural network to learn in some cases and, in fact, \n",
    "the following activation functions will change this part. This is also refer as zero-sparsity: \n",
    "a sparse network has neurons with few connections.\n",
    "As long as values are above zero, regardless of how large it is, the gradient of the activation function will be 1, \n",
    "meaning that it can learn anyways. This solves the vanishing gradient problem present in the sigmoid activation function \n",
    "\n",
    "LeakyRelu: The motivation for using LReLU instead of ReLU is that constant zero gradients can also result in slow learning, \n",
    "as when a saturated neuron uses a sigmoid activation function. Furthermore, some of them may not even activate. \n",
    "This sacrifice of the zero-sparsity, according to the authors, can provide worse results than when the neurons are completely \n",
    "deactivated (ReLU) In fact, the authors report the same or insignificantly better results when using PReLU instead of ReLU.\n",
    "\n",
    "Relu: When the synapse activity is zero it makes sense that the derivative of the activation function is zero \n",
    "because there is no need to update as the synapse was not used. Furthermore, if the value is lower than zero, \n",
    "the resulting derivative will be also zero leading to a disconnection of the neuron (no update). \n",
    "This is a good idea since disconnecting some neurons may reduce overfitting (as co-dependence is reduced), \n",
    "however this will hinder the neural network to learn in some cases and, in fact, the following activation functions will change \n",
    "this part. This is also refer as zero-sparsity: a sparse network has neurons with few connections.\n",
    "As long as values are above zero, regardless of how large it is, the gradient of the activation function will be 1, \n",
    "meaning that it can learn anyways. This solves the vanishing gradient problem present in the sigmoid activation function \n",
    "\n",
    "tanh: same shape as sigmoid but, the range of y is -1 to 1 . it's a reshaped version of sigmoid.\n",
    "\n",
    "sigmoid: It's used in binary classification.\n",
    "\n",
    "softmax: It normalizes y to 0 -1. the sum of y is 1. High value of Y means high probabilty of being the Y. It is used in \n",
    "multi-classification.\n",
    "\n",
    "4.momentumOptimizer을 사용할 때 momentum 하이퍼파라미터를 너무 1에 가깝게 하면 (예를 들면 0.9999) 어 떤 일이 일어날까요?\n",
    "하이퍼 파라미터가 1에 가까울 수록, 한번 업데이트 시킬때 극단으로 가버려서 우리가 원하는 값을 지나칠 수 있다. 다시 반대로 올때에도 \n",
    "극단적으로 변하기 때문에 섬세한 위치 조정이 어렵다.\n",
    "\n",
    "5.드롭아웃이 훈련 속도를 느리게 만드나요? predict(test셋 예측)도 느리게 만드나요?\n",
    "Dropout is a regularization technique, and is most effective at preventing overfitting. \n",
    "However, there are several places when dropout can hurt performance.\n",
    "Right before the last layer. This is generally a bad place to apply dropout, because the network has no ability to \n",
    "\"correct\" errors induced by dropout before the classification happens. If I read correctly, you might have put dropout \n",
    "right before the softmax in the iris MLP.\n",
    "When the network is small relative to the dataset, regularization is usually unnecessary. If the model capacity is already low, \n",
    "lowering it further by adding regularization will hurt performance. I noticed most of your networks were relatively small and shallow.\n",
    "When training time is limited. It's unclear if this is the case here, but if you don't train until convergence, dropout \n",
    "may give worse results. Usually dropout hurts performance at the start of training, but results in the final ''converged'' \n",
    "error being lower. Therefore, if you don't plan to train until convergence, you may not want to use dropout.\n",
    "Finally, I want to mention that as far as I know, dropout is rarely used nowaways, having been supplanted by a technique \n",
    "known as batch normalization. Of course, that's not to say dropout isn't a valid and effective tool to try out.\n",
    "\n",
    "6.Adam optimizer가 좋은 이유는 무엇인가요?\n",
    "While momentum accelerates our search in direction of minima, RMSProp impedes our search in direction of oscillations.\n",
    "Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.function.py의 파일에 있는 함수들이 무엇인지 정의하고 그 코드들을 설명해주세요 또한 \n",
    "#그 함수를 저번주에 구현한 neralnet에 적용시켜 코드로 구현 해 보세요\n",
    "def initialize_adam(parameters) : #Adams 초기화!!\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "    ### START CODE HERE ### (approx. 4 lines)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n",
    "        v[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n",
    "        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v, s\n",
    "\n",
    "\n",
    "#모수들을 Adam으로 업데이트 하는 것\n",
    "def function1(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "\n",
    "    L = len(parameters) // 2 #nn에서 layer들의 개수             \n",
    "    v_corrected = {}# 첫번째 moement estimate이랑 python dict 초기화                  \n",
    "    s_corrected = {}# 두번째 moement estimate이랑 python dict 초기화           \n",
    "    \n",
    " #모든 모수에 대하여 Adam 업데이트를 함 \n",
    "    for l in range(L):\n",
    "# 그레디언트의 평균값을 움직이는 것. input: v, grads, beta1 output: v\n",
    "       \n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1-beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1-beta1) * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "\n",
    "#bias 고쳐진 첫번째 moment estimate을 구하는 것. input: v, beta1, t output: v_corrected\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-np.power(beta1,t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-np.power(beta1,t))\n",
    "#squared gradients의 평균값을 움직이는 것. inputs: s, grads, beta2 output: s\n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1-beta2) * np.square(grads[\"dW\" + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1-beta2) * np.square(grads[\"db\" + str(l+1)])\n",
    "#bias 고쳐진 두번째 moment estimate을 우하는 것 inputs: s ,beta2,t output: s_corrected\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-np.power(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-np.power(beta2,t))\n",
    "#모수 업데이트 하는 것 input: parameters, Learning_rate, v_corrected, s_corrected\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    " \n",
    "\n",
    "\n",
    "    return parameters, v, s\n",
    "\n",
    "#foward propagation이랑 dropout을 하는 것.\n",
    "def function2(X, parameters, keep_prob = 0.5):\n",
    "\n",
    "    \n",
    "    np.random.seed(1)\n",
    "#파라미터 가져오는 것    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    #matrix initializing\n",
    "    D1 = np.random.rand(A1.shape[0],A1.shape[1])    \n",
    "    #D1의 자료를 0 또는 1 로 바꿔줌 .keep_prob을 기준으로\n",
    "    D1 = D1 < keep_prob  \n",
    "    #A1중에서 필요없는거 다 shut down 해주는 것.\n",
    "    A1 = np.multiply(A1,D1)   \n",
    "    #rescaling 살아남은 애들에게 가중치를 2배를 주는 것\n",
    "    A1 = A1/keep_prob                                \n",
    "    Z2 = np.dot(W2, A1) + b2 \n",
    "    A2 = relu(Z2)\n",
    "    #matrix initializing\n",
    "    D2 = np.random.rand(A2.shape[0],A2.shape[1])        \n",
    "    #D2의 자료를 0 또는 1로 바꿔줌. keep_prob을 기준으로\n",
    "    D2 = D2 < keep_prob\n",
    "    #A2중에서 필요없는거 다 shut down 해주는 것.\n",
    "    A2 = np.multiply(A2,D2)   \n",
    "    #rescaling\n",
    "    A2 = A2/keep_prob                                 \n",
    " \n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#baskward propagation이랑 dropout하는 것.\n",
    "def function3(X, Y, cache, keep_prob):\n",
    "\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    #forward propagation에서 shutdown 한 것들을 똑같이 D2에서 shutdown해주는것.\n",
    "    dA2 = np.multiply(dA2,D2) \n",
    "    #rescaling 살아남은 애들에게 가중치를 2배를 주는 것\n",
    "    dA2 = dA2/keep_prob              \n",
    "\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    #foward propagation에서 shutdown 한 것들을 똑같이 D1에서 shutdown 해주는것.\n",
    "    dA1 = np.multiply(dA1,D1)      \n",
    "    #rescaling\n",
    "    dA1 = dA1/keep_prob             \n",
    "\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.function2.py에 있는 함수가 무엇인지 정의하고, return값에 대한 설명을 해주세요\n",
    "#batch normalization forward\n",
    "def function1(x, gamma, beta, eps):\n",
    "    N, D = x.shape\n",
    "    #평균 값 구하는 것\n",
    "    mu = 1./N * np.sum(x, axis = 0)\n",
    "    #모든 값에서 mean을 빼는 것\n",
    "    xmu = x - mu\n",
    "    #분모를 구하는 것.\n",
    "    sq = xmu ** 2\n",
    "    #variance 구하는 것\n",
    "    var = 1./N * np.sum(sq, axis = 0)\n",
    "    #add eps for numerical stability, 그리고 제곱근 구함.\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "    #역수 취하기\n",
    "    ivar = 1./sqrtvar\n",
    "    #normalization 실행\n",
    "    xhat = xmu * ivar\n",
    "    gammax = gamma * xhat\n",
    "    out = gammax + beta\n",
    "    #값 저장하기.\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "#batch normalization backward\n",
    "def function2(dout, cache):\n",
    "    #cache에서 값들 가져오기\n",
    "    xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "    #input과 output의 차원 가져오기\n",
    "    N,D = dout.shape\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgammax = dout \n",
    "    dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "    #중간 partial derivatives 이아래부터는 derivative 구하는 수식을 넣은 거임.\n",
    "    dxhat = dgammax * gamma\n",
    "    divar = np.sum(dxhat*xmu, axis=0)\n",
    "    dxmu1 = dxhat * ivar\n",
    "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "    dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "    dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "    dxmu2 = 2 * xmu * dsq\n",
    "    dx1 = (dxmu1 + dxmu2)\n",
    "    dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "    dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "    dx = dx1 + dx2\n",
    "#x, gamma, beta에 대하여 gredient 구해준것.\n",
    "    return dx, dgamma, dbeta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
